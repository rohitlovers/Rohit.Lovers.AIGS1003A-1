{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19cb2aa2-64af-44a5-9708-c85a7d6aeb13",
   "metadata": {},
   "source": [
    "#Question 1\n",
    "To calculate the probabilities P(sunny|a cone of ice cream) and P(rainy|a cup of hot coffee) using Bayes' Theorem and the Naïve Bayes assumption, we can decompose the joint distribution with conditional independence. Let's break down the calculations:\n",
    "\n",
    "1. P(sunny|a cone of ice cream) =\n",
    "   We want to find the probability of the category \"Sunny\" given the statement \"a cone of ice cream.\"\n",
    "\n",
    "   According to Bayes' Theorem:\n",
    "   P(sunny|a cone of ice cream) ∝ P(a cone of ice cream|sunny) * P(sunny)\n",
    "\n",
    "   With the Naïve Bayes assumption:\n",
    "   P(a cone of ice cream|sunny) = P(a|sunny) * P(cone|sunny) * P(of|sunny) * P(ice|sunny) * P(cream|sunny)\n",
    "\n",
    "2. P(rainy|a cup of hot coffee) =\n",
    "   Similarly, we want to find the probability of the category \"Rainy\" given the statement \"a cup of hot coffee.\"\n",
    "\n",
    "   Using Bayes' Theorem:\n",
    "   P(rainy|a cup of hot coffee) ∝ P(a cup of hot coffee|rainy) * P(rainy)\n",
    "\n",
    "   With the Naïve Bayes assumption:\n",
    "   P(a cup of hot coffee|rainy) = P(a|rainy) * P(cup|rainy) * P(of|rainy) * P(hot|rainy) * P(coffee|rainy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7d6375c2-071f-4ca5-8d0d-bc9da3a6401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import classificationMethod\n",
    "\n",
    "class MostFrequentClassifier(classificationMethod.ClassificationMethod):\n",
    "  \"\"\"\n",
    "  The MostFrequentClassifier is a very simple classifier: for\n",
    "  every test instance presented to it, the classifier returns\n",
    "  the label that was seen most often in the training data.\n",
    "  \"\"\"\n",
    "  def __init__(self, legalLabels):\n",
    "    self.guess = None\n",
    "    self.type = \"mostfrequent\"\n",
    "  \n",
    "  def train(self, data, labels, validationData, validationLabels):\n",
    "    \"\"\"\n",
    "    Find the most common label in the training data.\n",
    "    \"\"\"\n",
    "    counter = util.Counter()\n",
    "    counter.incrementAll(labels, 1)\n",
    "    self.guess = counter.argMax()\n",
    "  \n",
    "  def classify(self, testData):\n",
    "    \"\"\"\n",
    "    Classify all test data as the most common label.\n",
    "    \"\"\"\n",
    "    return [self.guess for i in testData]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7138dee0-897d-4bf7-9923-19e9ed91340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import classificationMethod\n",
    "import math\n",
    "\n",
    "class NaiveBayesClassifier(classificationMethod.ClassificationMethod):\n",
    "    def __init__(self, legalLabels):\n",
    "        self.legalLabels = legalLabels\n",
    "        self.type = \"naivebayes\"\n",
    "        self.k = 1  # this is the smoothing parameter\n",
    "        self.automaticTuning = False\n",
    "        self.class_probs = {}  \n",
    "        \n",
    "    def setSmoothing(self, k):\n",
    "        self.k = k\n",
    "\n",
    "    def train(self, trainingData, trainingLabels, validationData, validationLabels):\n",
    "        self.features = list(trainingData[0].keys())\n",
    "\n",
    "        if self.automaticTuning:\n",
    "            kgrid = [0.001, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 20, 50]\n",
    "        else:\n",
    "            kgrid = [self.k]\n",
    "\n",
    "            self.trainAndTune(trainingData, trainingLabels, validationData, validationLabels, kgrid)\n",
    "\n",
    "    def trainAndTune(self, trainingData, trainingLabels, validationData, validationLabels, kgrid):\n",
    "        best_accuracy = 0.0\n",
    "        best_k = None\n",
    "        original_k = self.k  # Store the original k value\n",
    "\n",
    "        for k in kgrid:\n",
    "            # Train the classifier with Laplace smoothing\n",
    "            self.k = k\n",
    "            self.trainNaiveBayes(trainingData, trainingLabels)\n",
    "\n",
    "            # Calculate accuracy on validation data\n",
    "            accuracy = self.validate(validationData, validationLabels)\n",
    "\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_k = k\n",
    "\n",
    "        self.k = best_k  # Set the best 'k' value\n",
    "        self.k = original_k  # Reset to the original k value\n",
    "\n",
    "class NaiveBayesClassifier(classificationMethod.ClassificationMethod):\n",
    "    def trainNaiveBayes(self, trainingData, trainingLabels):\n",
    "        \n",
    "        self.class_probs = {label: 0.0 for label in self.legalLabels}\n",
    "        \n",
    "        self.feature_probs = {label: {feature: 0.0 for feature in self.features} for label in self.legalLabels}\n",
    "\n",
    "        total_count = len(trainingLabels)\n",
    "\n",
    "        # Collect class counts\n",
    "        class_counts = {label: 0 for label in self.legalLabels}\n",
    "        for label_ in trainingLabels:\n",
    "            class_counts[label_] += 1\n",
    "\n",
    "        # Calculate class probabilities\n",
    "        for label in self.legalLabels:\n",
    "            self.class_probs[label] = (class_counts[label] + self.k) / (total_count + self.k * len(self.legalLabels))\n",
    "\n",
    "        # Calculate feature probabilities\n",
    "        for label in self.legalLabels:\n",
    "            label_data = [datum for datum, label_ in zip(trainingData, trainingLabels) if label_ == label]\n",
    "            label_word_counts = {feature: 0 for feature in self.features}\n",
    "            for datum in label_data:\n",
    "                for feature, count in datum.items():\n",
    "                    label_word_counts[feature] += count\n",
    "            for feature in self.features:\n",
    "                self.feature_probs[label][feature] = (label_word_counts[feature] + self.k) / (\n",
    "                        sum(label_word_counts.values()) + self.k * len(self.features))\n",
    "\n",
    "    def validate(self, validationData, validationLabels):\n",
    "        correct = 0\n",
    "        total = len(validationLabels)\n",
    "\n",
    "        for datum, true_label in zip(validationData, validationLabels):\n",
    "            \n",
    "            predicted_label = self.classify(datum)\n",
    "            \n",
    "            if predicted_label == true_label:\n",
    "                correct += 1\n",
    "\n",
    "        accuracy = correct / total\n",
    "        return accuracy\n",
    "\n",
    "    def classify(self, testData):\n",
    "        guesses = []\n",
    "\n",
    "        for datum in testData:\n",
    "            \n",
    "            max_prob = float(\"-inf\")\n",
    "            best_label = None\n",
    "\n",
    "            for label in self.legalLabels:\n",
    "                prob = math.log(self.class_probs[label])\n",
    "\n",
    "                for feature, count in datum.items():\n",
    "                    if feature in self.feature_probs[label]:\n",
    "                        prob += count * math.log(self.feature_probs[label][feature])\n",
    "\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    best_label = label\n",
    "\n",
    "            guesses.append(best_label)\n",
    "\n",
    "            return guesses\n",
    "\n",
    "    def calculateLogJointProbabilities(self, datum):\n",
    "        logJoint = {label: 0.0 for label in self.legalLabels}\n",
    "\n",
    "        for label in self.legalLabels:\n",
    "            logJoint[label] = math.log(self.class_probs[label])\n",
    "\n",
    "            for feature, count in datum.items():\n",
    "                if feature in self.feature_probs[label]:\n",
    "                    logJoint[label] += count * math.log(self.feature_probs[label][feature])\n",
    "\n",
    "        return logJoint\n",
    "\n",
    "    def findHighOddsFeatures(self, label1, label2):\n",
    "        featuresOdds = []\n",
    "\n",
    "        for feature in self.features:\n",
    "            odds_ratio = (self.feature_probs[label1][feature] + self.k) / (\n",
    "                    self.feature_probs[label2][feature] + self.k)\n",
    "            featuresOdds.append((feature, odds_ratio))\n",
    "\n",
    "        featuresOdds.sort(key=lambda x: -x[1])\n",
    "        return [feature for feature, _ in featuresOdds[:100]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29769c37-520b-478f-a184-83ad3f488c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains feature extraction methods and harness \n",
    "# code for data classification\n",
    "\n",
    "import mostFrequent\n",
    "import naiveBayes\n",
    "import samples\n",
    "import sys\n",
    "import util\n",
    "\n",
    "TEST_SET_SIZE = 140\n",
    "DIGIT_DATUM_WIDTH=28\n",
    "DIGIT_DATUM_HEIGHT=28\n",
    "FACE_DATUM_WIDTH=60\n",
    "FACE_DATUM_HEIGHT=70\n",
    "\n",
    "\n",
    "def basicFeatureExtractorDigit(datum):\n",
    "  \"\"\"\n",
    "  Returns a set of pixel features indicating whether\n",
    "  each pixel in the provided datum is white (0) or gray/black (1)\n",
    "  \"\"\"\n",
    "  a = datum.getPixels()\n",
    "\n",
    "  features = util.Counter()\n",
    "  for x in range(DIGIT_DATUM_WIDTH):\n",
    "    for y in range(DIGIT_DATUM_HEIGHT):\n",
    "      if datum.getPixel(x, y) > 0:\n",
    "        features[(x,y)] = 1\n",
    "      else:\n",
    "        features[(x,y)] = 0\n",
    "  return features\n",
    "\n",
    "\n",
    "def analysis(classifier, guesses, testLabels, testData, rawTestData, printImage):\n",
    "  \"\"\"\n",
    "  This function is called after learning.\n",
    "  Include any code that you want here to help you analyze your results.\n",
    "  \n",
    "  Use the printImage(<list of pixels>) function to visualize features.\n",
    "  \n",
    "  An example of use has been given to you.\n",
    "  \n",
    "  - classifier is the trained classifier\n",
    "  - guesses is the list of labels predicted by your classifier on the test set\n",
    "  - testLabels is the list of true labels\n",
    "  - testData is the list of training datapoints (as util.Counter of features)\n",
    "  - rawTestData is the list of training datapoints (as samples.Datum)\n",
    "  - printImage is a method to visualize the features \n",
    "  (see its use in the odds ratio part in runClassifier method)\n",
    "  \n",
    "  This code won't be evaluated. It is for your own optional use\n",
    "  (and you can modify the signature if you want).\n",
    "  \"\"\"\n",
    "  \n",
    "  # Put any code here...\n",
    "  mistake_count = 0  # Track the number of mistakes found\n",
    "  for i in range(len(guesses)):\n",
    "      prediction = guesses[i]\n",
    "      truth = testLabels[i]\n",
    "      if prediction != truth:\n",
    "          print(\"===================================\")\n",
    "          print(\"Mistake on example %d\" % i)\n",
    "          print(\"Predicted %d; truth is %d\" % (prediction, truth))\n",
    "          print(\"Image:\")\n",
    "          print(rawTestData[i])\n",
    "          mistake_count += 1\n",
    "          \n",
    "      if mistake_count >= 4:\n",
    "          break  # Stop after finding the first four mistakes\n",
    "\n",
    "\n",
    "\n",
    "## =====================\n",
    "## You don't have to modify any code below.\n",
    "## =====================\n",
    "\n",
    "\n",
    "class ImagePrinter:\n",
    "    def __init__(self, width, height):\n",
    "      self.width = width\n",
    "      self.height = height\n",
    "\n",
    "def default(str):\n",
    "  return str + ' [Default: %default]'\n",
    "\n",
    "def readCommand(argv):\n",
    "  \"Processes the command used to run from the command line.\"\n",
    "  from optparse import OptionParser  \n",
    "  parser = OptionParser(USAGE_STRING)\n",
    "  \n",
    "  parser.add_option('-c', '--classifier', help=default('The type of classifier'), choices=['mostFrequent', 'nb', 'naiveBayes', 'perceptron', 'mira', 'minicontest'], default='mostFrequent')\n",
    "  parser.add_option('-d', '--data', help=default('Dataset to use'), choices=['digits', 'faces'], default='digits')\n",
    "  parser.add_option('-t', '--training', help=default('The size of the training set'), default=100, type=\"int\")\n",
    "  parser.add_option('-a', '--autotune', help=default(\"Whether to automatically tune hyperparameters\"), default=False, action=\"store_true\")\n",
    "  parser.add_option('-i', '--iterations', help=default(\"Maximum iterations to run training\"), default=3, type=\"int\")\n",
    "\n",
    "  options, otherjunk = parser.parse_args(argv)\n",
    "  if len(otherjunk) != 0:\n",
    "      raise Exception('Command line input not understood: ' + str(otherjunk))\n",
    "                  \n",
    "  args = {}\n",
    "  \n",
    "  # Set up variables according to the command line input.\n",
    "  print(\"Doing classification\")\n",
    "  print(\"--------------------\")\n",
    "  print(\"data:\\t\\t\" + options.data)\n",
    "  print(\"classifier:\\t\\t\" + options.classifier)\n",
    "  print(\"training set size:\\t\" + str(options.training))\n",
    "  if(options.data==\"digits\"):\n",
    "    printImage = ImagePrinter(DIGIT_DATUM_WIDTH, DIGIT_DATUM_HEIGHT)\n",
    "    featureFunction = basicFeatureExtractorDigit    \n",
    "  else:\n",
    "    print(\"Unknown dataset\", options.data)\n",
    "    print(USAGE_STRING)\n",
    "    sys.exit(2)\n",
    "    \n",
    "  if(options.data==\"digits\"):\n",
    "    legalLabels = list(range(10))\n",
    "  else:\n",
    "    legalLabels = list(range(2))\n",
    "    \n",
    "  if options.training <= 0:\n",
    "      print(\"Training set size should be a positive integer (you provided: %d)\" % options.training)\n",
    "      print(USAGE_STRING)\n",
    "      sys.exit(2)\n",
    "\n",
    "  if(options.classifier == \"mostFrequent\"):\n",
    "    classifier = mostFrequent.MostFrequentClassifier(legalLabels)\n",
    "  elif(options.classifier == \"naiveBayes\" or options.classifier == \"nb\"):\n",
    "    classifier = naiveBayes.NaiveBayesClassifier(legalLabels)\n",
    "    if (options.autotune):\n",
    "        print(\"using automatic tuning for naivebayes\")\n",
    "        classifier.automaticTuning = True\n",
    "  else:\n",
    "    print(\"Unknown classifier:\", options.classifier)\n",
    "    print(USAGE_STRING)\n",
    "    \n",
    "    sys.exit(2)\n",
    "\n",
    "  args['classifier'] = classifier\n",
    "  args['featureFunction'] = featureFunction\n",
    "  args['printImage'] = printImage\n",
    "  \n",
    "  return args, options\n",
    "\n",
    "USAGE_STRING = \"\"\"\n",
    "  USAGE:      python dataClassifier.py <options>\n",
    "  EXAMPLES:   (1) python dataClassifier.py\n",
    "                  - trains the default mostFrequent classifier on the digit dataset\n",
    "                  using the default 100 training examples and\n",
    "                  then test the classifier on test data\n",
    "                 \"\"\"\n",
    "\n",
    "# Main harness code\n",
    "\n",
    "def runClassifier(args, options):\n",
    "\n",
    "  featureFunction = args['featureFunction']\n",
    "  classifier = args['classifier']\n",
    "  printImage = args['printImage']\n",
    "      \n",
    "  # Load data  \n",
    "  numTraining = options.training\n",
    "\n",
    "  rawTrainingData = samples.loadDataFile(\"trainingimages\", numTraining,DIGIT_DATUM_WIDTH,DIGIT_DATUM_HEIGHT)\n",
    "  trainingLabels = samples.loadLabelsFile(\"traininglabels\", numTraining)\n",
    "  rawValidationData = samples.loadDataFile(\"validationimages\", TEST_SET_SIZE,DIGIT_DATUM_WIDTH,DIGIT_DATUM_HEIGHT)\n",
    "  validationLabels = samples.loadLabelsFile(\"validationlabels\", TEST_SET_SIZE)\n",
    "  rawTestData = samples.loadDataFile(\"testimages\", TEST_SET_SIZE,DIGIT_DATUM_WIDTH,DIGIT_DATUM_HEIGHT)\n",
    "  testLabels = samples.loadLabelsFile(\"testlabels\", TEST_SET_SIZE)\n",
    "    \n",
    "  \n",
    "  # Extract features\n",
    "  print(\"Extracting features...\")\n",
    "  trainingData = list(map(featureFunction, rawTrainingData))\n",
    "  validationData = list(map(featureFunction, rawValidationData))\n",
    "  testData = list(map(featureFunction, rawTestData))\n",
    "  \n",
    "  # Conduct training and testing\n",
    "  print(\"Training...\")\n",
    "  classifier.train(trainingData, trainingLabels, validationData, validationLabels)\n",
    "  print(\"Validating...\")\n",
    "  guesses = classifier.classify(validationData)\n",
    "  correct = [guesses[i] == validationLabels[i] for i in range(len(validationLabels))].count(True)\n",
    "  print(str(correct), (\"correct out of \" + str(len(validationLabels)) + \" (%.1f%%).\") % (100.0 * correct / len(validationLabels)))\n",
    "  print(\"Testing...\")\n",
    "  guesses = classifier.classify(testData)\n",
    "  correct = [guesses[i] == testLabels[i] for i in range(len(testLabels))].count(True)\n",
    "  print(str(correct), (\"correct out of \" + str(len(testLabels)) + \" (%.1f%%).\") % (100.0 * correct / len(testLabels)))\n",
    "  analysis(classifier, guesses, testLabels, testData, rawTestData, printImage)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # Read input\n",
    "  args, options = readCommand( sys.argv[1:] ) \n",
    "  # Run classifier\n",
    "  runClassifier(args, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8ac6ffcc-8cc2-46b9-a860-4d8332664225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing classification\n",
      "--------------------\n",
      "data:\t\tdigits\n",
      "classifier:\t\tmostFrequent\n",
      "training set size:\t100\n",
      "Extracting features...\n",
      "Training...\n",
      "Validating...\n",
      "17 correct out of 140 (12.1%).\n",
      "Testing...\n",
      "18 correct out of 140 (12.9%).\n",
      "===================================\n",
      "Mistake on example 0\n",
      "Predicted 1; truth is 9\n",
      "Image:\n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "             ++###+         \n",
      "             ######+        \n",
      "            +######+        \n",
      "            ##+++##+        \n",
      "           +#+  +##+        \n",
      "           +##++###+        \n",
      "           +#######+        \n",
      "           +#######+        \n",
      "            +##+###         \n",
      "              ++##+         \n",
      "              +##+          \n",
      "              ###+          \n",
      "            +###+           \n",
      "            +##+            \n",
      "           +##+             \n",
      "          +##+              \n",
      "         +##+               \n",
      "         ##+                \n",
      "        +#+                 \n",
      "        +#+                 \n",
      "                            \n",
      "===================================\n",
      "Mistake on example 1\n",
      "Predicted 1; truth is 0\n",
      "Image:\n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "              +#++          \n",
      "             +####+         \n",
      "           ++######         \n",
      "           +###+++#         \n",
      "          +###+  +#+        \n",
      "          +###   +##        \n",
      "         +###+    ##+       \n",
      "         +##+     +#+       \n",
      "         ###      +#+       \n",
      "        +##+      +##+      \n",
      "        +##       +##+      \n",
      "        +#+       +##+      \n",
      "        +#+       +##       \n",
      "        +#+       +#+       \n",
      "        +##+     +##+       \n",
      "         ###+    +##+       \n",
      "         +####++###++       \n",
      "         +#########         \n",
      "          ++#######         \n",
      "            +###+++         \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "===================================\n",
      "Mistake on example 2\n",
      "Predicted 1; truth is 2\n",
      "Image:\n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "           +++#####+        \n",
      "          +#########+       \n",
      "          +##########+      \n",
      "           ++++  +###+      \n",
      "                  +##+      \n",
      "                 +###+      \n",
      "                +###+       \n",
      "               +###+        \n",
      "              +####+        \n",
      "              +###+         \n",
      "             +###+          \n",
      "            +###+           \n",
      "           +###+            \n",
      "          +###+             \n",
      "         +####+             \n",
      "        +####+  ++++        \n",
      "       +#####++###++        \n",
      "      +##########+          \n",
      "       +#######+            \n",
      "         ++++               \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "===================================\n",
      "Mistake on example 3\n",
      "Predicted 1; truth is 5\n",
      "Image:\n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "          +#########+       \n",
      "         +###########+      \n",
      "         ############+      \n",
      "         ############       \n",
      "         ####+++#####       \n",
      "         +##+     +++       \n",
      "         +###++++           \n",
      "          ########+         \n",
      "          #########+        \n",
      "          ##########+       \n",
      "          +##########       \n",
      "           +++  ++###+      \n",
      "                  +###      \n",
      "      ++           ###      \n",
      "     +###++       +###      \n",
      "      ######++   +###+      \n",
      "      ++#############       \n",
      "       ++###########+       \n",
      "         +#########+        \n",
      "           ++####++         \n",
      "                            \n",
      "                            \n",
      "                            \n"
     ]
    }
   ],
   "source": [
    "!python dataClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b9a4d563-52ea-4dfc-9ad5-80cc2f789759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: \n",
      "  USAGE:      python dataClassifier.py <options>\n",
      "  EXAMPLES:   (1) python dataClassifier.py\n",
      "                  - trains the default mostFrequent classifier on the digit dataset\n",
      "                  using the default 100 training examples and\n",
      "                  then test the classifier on test data\n",
      "                 \n",
      "\n",
      "Options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -c CLASSIFIER, --classifier=CLASSIFIER\n",
      "                        The type of classifier [Default: mostFrequent]\n",
      "  -d DATA, --data=DATA  Dataset to use [Default: digits]\n",
      "  -t TRAINING, --training=TRAINING\n",
      "                        The size of the training set [Default: 100]\n",
      "  -a, --autotune        Whether to automatically tune hyperparameters\n",
      "                        [Default: False]\n",
      "  -i ITERATIONS, --iterations=ITERATIONS\n",
      "                        Maximum iterations to run training [Default: 3]\n"
     ]
    }
   ],
   "source": [
    "!python dataClassifier.py -h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fdd7ba27-97ca-4cba-811b-d6ff0a506379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing classification\n",
      "--------------------\n",
      "data:\t\tdigits\n",
      "classifier:\t\tnaiveBayes\n",
      "training set size:\t100\n",
      "using automatic tuning for naivebayes\n",
      "Extracting features...\n",
      "Training...\n",
      "Performance validation set for k=0.001: 66.4%\n",
      "Performance validation set for k=0.010: 67.9%\n",
      "Performance validation set for k=0.050: 70.0%\n",
      "Performance validation set for k=0.100: 70.7%\n",
      "Performance validation set for k=0.500: 68.6%\n",
      "Performance validation set for k=1.000: 67.9%\n",
      "Performance validation set for k=5.000: 55.0%\n",
      "Performance validation set for k=10.000: 48.6%\n",
      "Performance validation set for k=20.000: 36.4%\n",
      "Performance validation set for k=50.000: 26.4%\n",
      "Validating...\n",
      "99 correct out of 140 (70.7%).\n",
      "Testing...\n",
      "81 correct out of 140 (57.9%).\n",
      "===================================\n",
      "Mistake on example 3\n",
      "Predicted 3; truth is 5\n",
      "Image:\n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "          +#########+       \n",
      "         +###########+      \n",
      "         ############+      \n",
      "         ############       \n",
      "         ####+++#####       \n",
      "         +##+     +++       \n",
      "         +###++++           \n",
      "          ########+         \n",
      "          #########+        \n",
      "          ##########+       \n",
      "          +##########       \n",
      "           +++  ++###+      \n",
      "                  +###      \n",
      "      ++           ###      \n",
      "     +###++       +###      \n",
      "      ######++   +###+      \n",
      "      ++#############       \n",
      "       ++###########+       \n",
      "         +#########+        \n",
      "           ++####++         \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "===================================\n",
      "Mistake on example 7\n",
      "Predicted 9; truth is 8\n",
      "Image:\n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "            ++#+            \n",
      "           +####+           \n",
      "          +##++##+          \n",
      "          ##+ +###+         \n",
      "         +##+  ++#+         \n",
      "         +##+   +#+         \n",
      "         +##+  +##+         \n",
      "         +##++ +##          \n",
      "          +##++##+          \n",
      "           +#####           \n",
      "            ####+           \n",
      "             ###+           \n",
      "            +####           \n",
      "            ##+##+          \n",
      "           +## +##          \n",
      "           +#+ +#+          \n",
      "           ##+++#+          \n",
      "          +##+###+          \n",
      "          +####++           \n",
      "          +#+++             \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "===================================\n",
      "Mistake on example 12\n",
      "Predicted 9; truth is 7\n",
      "Image:\n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "           ++##++           \n",
      "        +#+######           \n",
      "        #####++##           \n",
      "        ++++   +#           \n",
      "               +#+          \n",
      "               +#+          \n",
      "               +#+          \n",
      "         +++++++#+          \n",
      "        +########+          \n",
      "         ++########+        \n",
      "            ++#####+        \n",
      "              +##+          \n",
      "              +#+           \n",
      "              +#+           \n",
      "              +#+           \n",
      "              +#+           \n",
      "              +#+           \n",
      "              +#+           \n",
      "              +#+           \n",
      "              +#            \n",
      "                            \n",
      "===================================\n",
      "Mistake on example 14\n",
      "Predicted 4; truth is 6\n",
      "Image:\n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                   +++++    \n",
      "              +++#######+   \n",
      "           ++###+++    +#   \n",
      "         ++##++        ++   \n",
      "        +#++          +#+   \n",
      "       ##+           ++     \n",
      "     +#+              +     \n",
      "     +#     +               \n",
      "     ++    +#++             \n",
      "     ##     ++##++          \n",
      "     +#++      ++#+         \n",
      "      +##++      +#+        \n",
      "        ++##++++##+         \n",
      "          ++++++++          \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n"
     ]
    }
   ],
   "source": [
    "!python dataClassifier.py -c naiveBayes --autotune"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
